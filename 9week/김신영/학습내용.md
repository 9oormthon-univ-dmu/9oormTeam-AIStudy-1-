## 9주차 - 차근차근 PyTorch & TorchVision - MLOps를 위한 모델 실험 추적 & 모델 웹앱 배포까지

## 9. Transfer-learning 
### 강의 키워드
EfficientNet, FocalLoss
### 강의내용
---

## EfficientNet과 모델 수정

### EfficientNet
- **개념**: EfficientNet은 Neural Architecture Search(NAS) 기법을 활용하여 설계된 CNN(Convolutional Neural Network) 모델입니다. 이 모델은 다양한 크기와 깊이를 가진 여러 변형을 제공하여, 특정 작업에 적합한 최적의 아키텍처를 선택할 수 있도록 합니다.
- **장점**: 작은 모델로도 높은 성능을 발휘하도록 설계되어 있어, 리소스가 제한된 환경에서도 효율적으로 사용할 수 있습니다. 또한, 성능 대비 파라미터 수가 적어 연산 속도와 메모리 사용 측면에서 우수합니다. 이는 모델을 훈련할 때 필요한 데이터 양을 줄여주고, 과적합(overfitting)을 방지하는 데 기여합니다.

### 모델 수정
- **분류 계층 제거**: EfficientNet의 기본 구조는 다양한 이미지 분류 작업을 수행할 수 있도록 설계되었습니다. 하지만 특정 문제에 맞게 사용하기 위해서는 기존의 분류 계층을 제거해야 합니다.
- **마지막 계층 수정**: 새로운 데이터셋에 맞춰 최종 출력 클래스를 조정합니다. 예를 들어, 새롭게 정의한 클래스 수에 따라 마지막 Fully Connected Layer를 재구성하여, 해당 클래스 수에 맞는 출력을 제공하도록 합니다. 이를 통해 모델은 특정 작업에 대한 최적화된 예측을 수행할 수 있습니다.

---

## 손실 함수 및 최적화기 설정

### FocalLoss
- **개념**: Focal Loss는 클래스 불균형 문제를 해결하기 위해 설계된 손실 함수입니다. 일반적인 Cross-Entropy Loss에 비해 잘못 분류된 샘플에 더 많은 가중치를 두어 학습하도록 합니다.
- **작동 방식**: 주로 어려운 샘플에 집중하도록 설계되어, 모델이 쉽게 분류할 수 있는 샘플은 낮은 가중치를, 어렵게 분류되는 샘플은 높은 가중치를 부여합니다. 이를 통해, 불균형한 데이터셋에서도 효과적으로 학습할 수 있습니다.
- **적용 사례**: 클래스 불균형이 심한 데이터셋(예: 특정 클래스의 샘플 수가 현저히 적은 경우)에서 성능을 개선하는 데 매우 유용합니다.

### AdamW
- **개념**: AdamW는 Adam 최적화기의 변형으로, weight decay(가중치 감소)를 추가한 버전입니다. Adam은 Adaptive Moment Estimation의 약자로, 학습률을 각 파라미터에 따라 다르게 조정하여 효율적으로 학습합니다.
- **장점**: AdamW는 weight decay를 적용하여 일반화 성능을 개선합니다. 이는 과적합을 방지하고, 모델이 훈련 데이터에 지나치게 의존하지 않도록 도와줍니다. 
- **운영 방식**: AdamW는 최적화 과정에서 가중치를 업데이트할 때, weight decay를 적용하여 파라미터를 조정합니다. 이를 통해, 전체 모델의 성능을 높이고, 다양한 데이터셋에서 더 나은 결과를 얻을 수 있습니다.

---
